<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Centralized Logging Setup using Ansible (ELK Stack) – End-to-End Project Implementation</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: "Courier New", Courier, monospace;
            background: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
        }
    </style>
</head>
<body>
    <h1>Centralized Logging Setup using Ansible (ELK Stack) – End-to-End Project Implementation</h1>
    
    <h2>Task Overview</h2>
    <p>This Task involves setting up a centralized logging system using the <strong>ELK stack (Elasticsearch, Logstash, Kibana)</strong> and deploying <strong>Filebeat</strong> as a log shipper to forward logs from multiple application servers. The entire process will be automated using <strong>Ansible</strong>, ensuring a consistent, scalable, and efficient deployment.</p>
    
    <h2>Technology Stack</h2>
    <ul>
        <li><strong>Ansible</strong> – Automate the deployment of the ELK stack and log shippers.</li>
        <li><strong>Elasticsearch</strong> – Store and index logs for querying.</li>
        <li><strong>Logstash</strong> – Process and parse incoming logs.</li>
        <li><strong>Kibana</strong> – Visualize and analyze logs.</li>
        <li><strong>Filebeat</strong> – Lightweight shipper to forward logs to Logstash.</li>
        <li><strong>Linux (Ubuntu/CentOS)</strong> – OS for hosting ELK and Filebeat.</li>
    </ul>
    
    <h2>Project Architecture</h2>
    <ul>
        <li><strong>Application Servers</strong> – Run various applications generating logs.</li>
        <li><strong>Filebeat Agents</strong> – Installed on application servers to collect and forward logs.</li>
        <li><strong>Logstash Server</strong> – Processes and enriches logs before sending them to Elasticsearch.</li>
        <li><strong>Elasticsearch Cluster</strong> – Stores and indexes logs for fast querying.</li>
        <li><strong>Kibana</strong> – Provides a UI for log visualization and analysis.</li>
    </ul>
    
    <h2>Step-by-Step Implementation using Ansible</h2>
    
    <h3>Step 1: Set Up Inventory File</h3>
    <p>Create an Ansible inventory file defining the ELK and client servers.</p>
    <pre><code>[elk]
elk-master ansible_host=192.168.1.100 ansible_user=ubuntu

[app_servers]
app1 ansible_host=192.168.1.101 ansible_user=ubuntu
app2 ansible_host=192.168.1.102 ansible_user=ubuntu</code></pre>
    
    <h3>Step 2: Create Ansible Playbooks</h3>
    <h4>1. Install and Configure Elasticsearch</h4>
    <p>Create a playbook install_elasticsearch.yml to install and configure Elasticsearch.</p>
    <pre><code>- hosts: elk
  become: yes
  tasks:
    - name: Install dependencies
      apt:
        name: [apt-transport-https, openjdk-11-jdk]
        state: present

    - name: Add Elasticsearch GPG Key
      shell: wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -

    - name: Add Elasticsearch repository
      copy:
        dest: /etc/apt/sources.list.d/elastic-7.x.list
        content: "deb https://artifacts.elastic.co/packages/7.x/apt stable main"

    - name: Install Elasticsearch
      apt:
        name: elasticsearch
        state: present
        update_cache: yes

    - name: Configure Elasticsearch
      lineinfile:
        path: /etc/elasticsearch/elasticsearch.yml
        line: "{{ item }}"
      with_items:
        - "network.host: 0.0.0.0"
        - "http.port: 9200"
        - "cluster.name: elk-cluster"
        - "node.name: elk-master"

    - name: Start Elasticsearch service
      systemd:
        name: elasticsearch
        enabled: yes
        state: started</code></pre>
    <h4>2. Install and Configure Logstash</h4>
    <p>Create a playbook install_logstash.yml to install and configure Logstash.</p>
    <pre><code>- hosts: elk
  become: yes
  tasks:
    - name: Install Logstash
      apt:
        name: logstash
        state: present

    - name: Configure Logstash pipeline
      copy:
        dest: /etc/logstash/conf.d/logstash.conf
        content: |
          input {
            beats {
              port => 5044
            }
          }
          filter {
            mutate {
              add_field => { "log_source" => "%{host}" }
            }
          }
          output {
            elasticsearch {
              hosts => ["http://localhost:9200"]
              index => "logs-%{+YYYY.MM.dd}"
            }
          }

    - name: Restart Logstash
      systemd:
        name: logstash
        enabled: yes
        state: restarted
</code></pre>
    <h4>3. Install and Configure Kibana</h4>
    <p>Create a playbook install_kibana.yml to install and configure Kibana.</p>
    <pre><code>- hosts: elk
  become: yes
  tasks:
    - name: Install Kibana
      apt:
        name: kibana
        state: present

    - name: Configure Kibana
      lineinfile:
        path: /etc/kibana/kibana.yml
        line: "{{ item }}"
      with_items:
        - "server.host: '0.0.0.0'"
        - "elasticsearch.hosts: ['http://localhost:9200']"

    - name: Start Kibana service
      systemd:
        name: kibana
        enabled: yes
        state: started
</code></pre>
    <h4>4. Install and Configure Filebeat on Application Servers</h4>
    <p>Create a playbook install_filebeat.yml to install and configure Filebeat.</p>
    <pre><code>- hosts: app_servers
  become: yes
  tasks:
    - name: Download Filebeat
      get_url:
        url: https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.x-amd64.deb
        dest: /tmp/filebeat.deb

    - name: Install Filebeat
      shell: dpkg -i /tmp/filebeat.deb

    - name: Configure Filebeat
      copy:
        dest: /etc/filebeat/filebeat.yml
        content: |
          filebeat.inputs:
            - type: log
              paths:
                - /var/log/syslog
                - /var/log/auth.log
          output.logstash:
            hosts: ["192.168.1.100:5044"]

    - name: Start Filebeat
      systemd:
        name: filebeat
        enabled: yes
        state: started
</code></pre>
    <h3>Step 3: Run Ansible Playbooks</h3>
    <p>Run the playbooks in order to set up the centralized logging system.</p>
    <pre><code>ansible-playbook -i inventory install_elasticsearch.yml
ansible-playbook -i inventory install_logstash.yml
ansible-playbook -i inventory install_kibana.yml
ansible-playbook -i inventory install_filebeat.yml</code></pre>
    
    <h2>Validation and Testing</h2>
    <h3>1. Verify Elasticsearch</h3>
    <p>Run the following command on the ELK server:</p>
    <pre><code>curl -X GET "http://localhost:9200/_cluster/health?pretty"</code></pre>
    
    <h3>2. Verify Logstash</h3>
    <p>Check if Logstash is listening on port 5044:</p>
    <pre><code>netstat -tulnp | grep 5044</code></pre>
    
    <h3>3. Verify Kibana</h3>
    <p>Open a web browser and go to:</p>
    <pre><code>http://&lt;ELK_SERVER_IP&gt;:5601</code></pre>
    <p>Check if Kibana is loading.</p>
    
    <h3>4. Verify Logs in Kibana</h3>
    <ul>
      <li>Go to <strong>Kibana Dashboard > Discover.</strong></li>
      <li>Check if logs from application servers are visible.</li>
    </ul>
  
    <h2>Security Considerations</h2>
    <ul>
      <li>Restrict access to Elasticsearch, Kibana, and Logstash using <strong>firewall rules.</strong></li>
      <li>Configure <strong>TLS encryption</strong> for secure log transmission.</li>
      <li>Implement <strong>role-based access control (RBAC)</strong> in Kibana.</li>
    </ul>

    <h2>Future Enhancements</h2>
    <ul>
      <li>Implement <strong>Loki and Grafana</strong> for lightweight log monitoring.</li>
      <li>Deploy ELK in a <strong>Kubernetes environment</strong> using Helm charts.</li>
      <li>Configure <strong>Elasticsearch clustering</strong> for better scalability.</li>
    </ul>
    <h2>Conclusion</h2>
    <p>This project automates the deployment of a centralized logging system using Ansible, ensuring logs from multiple servers are 
      collected, processed, stored, and visualized efficiently. The solution improves troubleshooting, security auditing, and performance 
      monitoring across an enterprise infrastructure.</p>

    <h1>Enhancing Centralized Logging with AWS CloudWatch and S3 Using Ansible</h1>

    <h2>Project Overview</h2>
    <p>In this extension of the <strong>Centralized Logging Setup</strong>, we integrate <strong>AWS CloudWatch and S3</strong> for log storage, 
      monitoring, and alerting. This allows us to:</p>
    <ul>
        <li>Ship logs from <strong>Logstash/Filebeat</strong> to <strong>AWS CloudWatch Logs</strong>.</li>
        <li>Store logs in <strong>Amazon S3</strong> for long-term archival.</li>
        <li>Leverage <strong>CloudWatch Metrics & Alarms</strong> for monitoring log trends.</li>
        <li>Use <strong>IAM roles and policies</strong> to secure access.</li>
    </ul>

    <h2>Updated Architecture</h2>
    <ul>
        <li><strong>Application Servers</strong> – Generate logs.</li>
        <li><strong>Filebeat Agents</strong> – Forward logs to Logstash and CloudWatch.</li>
        <li><strong>Logstash Server</strong> – Processes logs and sends them to both <strong>CloudWatch Logs</strong> and <strong>Elasticsearch</strong>.</li>
        <li><strong>Elasticsearch Cluster</strong> – Stores logs for querying.</li>
        <li><strong>Kibana</strong> – Visualizes logs.</li>
        <li><strong>Amazon CloudWatch Logs</strong> – Centralized logging on AWS.</li>
        <li><strong>Amazon S3</strong> – Long-term log storage.</li>
    </ul>

    <h2>Step-by-Step Implementation</h2>
    
    <h3>Step 1: Set Up IAM Roles and Policies for CloudWatch and S3</h3>
    <p>We create an <strong>IAM user</strong> with permissions to push logs to <strong>CloudWatch Logs</strong> and store them in <strong>S3</strong>.</p>
    
    <h4>1.1 Create IAM Policy for Log Management</h4>
    <p>Use the following JSON policy for CloudWatch and S3 permissions.</p>
    <pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogStream",
                "logs:PutLogEvents",
                "logs:DescribeLogGroups",
                "logs:DescribeLogStreams",
                "logs:CreateLogGroup"
            ],
            "Resource": "arn:aws:logs:*:*:*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": "arn:aws:s3:::centralized-log-storage/*"
        }
    ]
}</code></pre>
    <p>Attach this policy to an IAM user and generate <strong>AWS_ACCESS_KEY_ID</strong> and <strong>AWS_SECRET_ACCESS_KEY</strong>.</p>

    <h3>Step 2: Install and Configure AWS CLI on Logstash Server</h3>
    <p>Run the following <strong>Ansible playbook</strong> to install the AWS CLI and configure credentials.</p>
    
    <h4>2.1 Create Playbook <code>install_aws_cli.yml</code></h4>
    <pre><code>- hosts: elk
  become: yes
  tasks:
    - name: Install AWS CLI
      apt:
        name: awscli
        state: present

    - name: Configure AWS Credentials
      template:
        src: aws_credentials.j2
        dest: /root/.aws/credentials
        mode: '0600'</code></pre>
    
    <h4>2.2 Create AWS Credentials Template <code>aws_credentials.j2</code></h4>
    <pre><code>[default]
aws_access_key_id = {{ aws_access_key }}
aws_secret_access_key = {{ aws_secret_key }}
region = us-east-1</code></pre>
    
    <p>Replace <code>aws_access_key</code> and <code>aws_secret_key</code> with actual AWS credentials in <code>ansible-playbook</code> command.</p>
    
    <h3>Step 3: Configure Logstash for CloudWatch and S3</h3>
    <p>We modify the <strong>Logstash pipeline</strong> to forward logs to AWS CloudWatch and S3.</p>
    
    <h4>3.1 Modify Logstash Configuration</h4>
    <pre><code>- hosts: elk
  become: yes
  tasks:
    - name: Configure Logstash for AWS CloudWatch and S3
      copy:
        dest: /etc/logstash/conf.d/logstash.conf
        content: |
          input {
            beats {
              port => 5044
            }
          }

          filter {
            mutate {
              add_field => { "log_source" => "%{host}" }
            }
          }

          output {
            elasticsearch {
              hosts => ["http://localhost:9200"]
              index => "logs-%{+YYYY.MM.dd}"
            }

            cloudwatch {
              log_group => "application-logs"
              log_stream_name => "%{log_source}"
              region => "us-east-1"
              access_key_id => "{{ aws_access_key }}"
              secret_access_key => "{{ aws_secret_key }}"
            }

            s3 {
              access_key_id => "{{ aws_access_key }}"
              secret_access_key => "{{ aws_secret_key }}"
              region => "us-east-1"
              bucket => "centralized-log-storage"
              codec => "json_lines"
              prefix => "logs/%{+YYYY}/%{+MM}/%{+dd}/"
            }
          }

    - name: Restart Logstash
      systemd:
        name: logstash
        enabled: yes
        state: restarted
</code></pre>
    <h3>Step 4: Configure Filebeat to Send Logs to CloudWatch (Optional)</h3>
    <p>Instead of using Logstash, we can configure Filebeat to push logs directly to AWS CloudWatch.</p>
    <h4>4.1 Modify Filebeat Configuration</h4>
    <pre><code>- hosts: app_servers
  become: yes
  tasks:
    - name: Configure Filebeat to send logs to AWS CloudWatch
      copy:
        dest: /etc/filebeat/filebeat.yml
        content: |
          filebeat.inputs:
            - type: log
              paths:
                - /var/log/syslog
                - /var/log/auth.log
          
          output.console:
            pretty: true

          output.awscloudwatch:
            log_group_name: "application-logs"
            log_stream_name: "%{[host]}"
            region: "us-east-1"
            access_key_id: "{{ aws_access_key }}"
            secret_access_key: "{{ aws_secret_key }}"

    - name: Restart Filebeat
      systemd:
        name: filebeat
        enabled: yes
        state: restarted
</code></pre>
    <h3>Step 5: Deploy and Verify</h3>
    <p>Run the following Ansible playbooks to apply changes.</p>
    <pre><code>ansible-playbook -i inventory install_aws_cli.yml
ansible-playbook -i inventory install_filebeat.yml
ansible-playbook -i inventory install_logstash.yml
</code></pre>
    <h3>Step 6: Validate CloudWatch and S3 Integration</h3>
    <h4>6.1 Check Logs in CloudWatch</h4>
    <pre><code>aws logs describe-log-groups --region us-east-1
</code></pre>
    <h4>6.2 Check Log Data in S3</h4>
    <pre><code>aws s3 ls s3://centralized-log-storage/logs/ --recursive
</code></pre>
    <h4>6.3 View Logs in AWS Console</h4>
    <ul>
      <li>Open AWS CloudWatch → Log Groups → application-logs.</li>
      <li>Open Amazon S3 Console → Bucket: centralized-log-storage → Check logs.</li>
    </ul>

    <h3>Step 7: Set Up CloudWatch Alarms</h3>
    <p>We configure CloudWatch Alarms to alert on error logs.</p>
    <h4>7.1 Create CloudWatch Metric Filter</h4>
    <pre><code>aws logs put-metric-filter \
    --log-group-name "application-logs" \
    --filter-name "ErrorFilter" \
    --filter-pattern "ERROR" \
    --metric-transformations metricName=ErrorCount,metricNamespace=LogMetrics,metricValue=1
</code></pre>
    <h4>7.2 Create CloudWatch Alarm</h4>
    <pre><code>aws cloudwatch put-metric-alarm \
    --alarm-name "HighErrorRate" \
    --metric-name "ErrorCount" \
    --namespace "LogMetrics" \
    --statistic "Sum" \
    --period 300 \
    --threshold 10 \
    --comparison-operator "GreaterThanThreshold" \
    --evaluation-periods 1 \
    --alarm-actions "arn:aws:sns:us-east-1:123456789012:SendAlert"
</code></pre>
    <p>This will trigger an alert if more than 10 error logs appear within 5 minutes.</p>

  <h3>Conclusion</h3>
  <ul>
    <li>Sending logs to AWS CloudWatch Logs for real-time monitoring.</li>
    <li>Storing logs in Amazon S3 for long-term archival.</li>
    <li>Automating deployment with Ansible.</li>
    <li>Implementing CloudWatch Alarms for proactive monitoring.</li>
  </ul>
</body>
</html>


